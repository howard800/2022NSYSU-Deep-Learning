{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYkZGeyR2au4"
      },
      "source": [
        "# MIS 583 Assignment 6 Part 1: Image Captioning"
      ],
      "id": "mYkZGeyR2au4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOPpE4vN2jc8"
      },
      "source": [
        "Before we start, please put your name and ID in following format: <br>\n",
        "NAME, ?000000000   e.g. 陳琨翔, M094020003\n",
        "\n",
        "**Your Answer:**   \n",
        "Hi I'm 王渙鈞, M114020052"
      ],
      "id": "YOPpE4vN2jc8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_MoQiztpxcK"
      },
      "source": [
        "## Image Captioning\n",
        "\n",
        "Image captioning is an interdisciplinary research problem that stands between computer vision and natural language processing.\n",
        "\n",
        "In the assignment, you will design and train a Image captioning network from scratch to process the input image, then output a sequence that describe the image. \n",
        "\n",
        "**Note that you are free to use pre-trained models like ResNet or LSTM as your backbone structure.**"
      ],
      "id": "B_MoQiztpxcK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giUId1Naqacs"
      },
      "source": [
        "##  Versions of used packages\n",
        "\n",
        "We will check PyTorch version to make sure everything work properly.\n",
        "\n",
        "I use `python 3.8.11`, `torch==1.8.2` and `torchvision==0.9.2`."
      ],
      "id": "giUId1Naqacs"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vuw-gNvjqcYe",
        "outputId": "6149302d-79ec-4bac-ed0d-292fb4f6b4ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.8.16 (default, Dec  7 2022, 01:12:13) \n",
            "torch 1.13.0+cu116\n",
            "torchvision 0.14.0+cu116\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "print('python', sys.version.split('\\n')[0])\n",
        "print('torch', torch.__version__)\n",
        "print('torchvision', torchvision.__version__)"
      ],
      "id": "Vuw-gNvjqcYe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG0ffPN76B_v"
      },
      "outputs": [],
      "source": [],
      "id": "tG0ffPN76B_v"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjapiBe36CL4"
      },
      "outputs": [],
      "source": [],
      "id": "LjapiBe36CL4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FydjP4MTWuL"
      },
      "source": [
        "## Error handling\n",
        "\n",
        "**RuntimeError: CUDA out of memory...**\n",
        "> 發生原因可能為讀取的 batch 過大或是記憶體未釋放乾淨。若縮小 batch size 後仍出現錯誤請按照以下步驟重新載入 colab。\n",
        "> The reason of this error is over-sized batch_size or unreleased memory. If changing the batch_size smaller still cause this error. Please reload colab follow below instructions.\n",
        "\n",
        "1. Click 「Runtime」\n",
        "2. Click 「Factor reset runtime」\n",
        "3. Click 「Reconnect」\n",
        "4. Reload all chunk\n",
        "\n"
      ],
      "id": "_FydjP4MTWuL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhdbdJOsrbxL"
      },
      "source": [
        "# Prepare Data"
      ],
      "id": "OhdbdJOsrbxL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPoSgD83teTQ"
      },
      "source": [
        "We use [Flickr8k](https://www.kaggle.com/dataset/e1cd22253a9b23b073794872bf565648ddbe4f17e7fa9e74766ad3707141adeb) dataset.\n",
        "This is collected by Alexander Mamaev.\n",
        "\n",
        "**Abstrct**  \n",
        "A new benchmark collection for sentence-based image description and search, consisting of 8,091 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. … The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations\n",
        "\n",
        "Photos are not reduced to a single size, they have different proportions!"
      ],
      "id": "nPoSgD83teTQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMThsYeDa2O"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## How to Get Data\n",
        "\n",
        "Please open the file `flickr8k.zip`, creat shortcut to your Google Drive.\n",
        "\n",
        "1. open [LINK of Google Drive](https://drive.google.com/file/d/1awvS-E5IPMJgu6kiRjZbiwyrmx6Cl6VO/view?usp=sharing)\n",
        "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
        "3. Select the location where you want to place the shortcut.\n",
        "4. Click Add shortcut.\n",
        "\n",
        "After above procedures, we have a shortcut of zip file of dataset.  \n",
        "We can access this in colab after granting the permission of Google Drive.\n",
        "\n",
        "---\n",
        "\n",
        "請先到共用雲端硬碟將檔案 `flickr8k.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/1awvS-E5IPMJgu6kiRjZbiwyrmx6Cl6VO/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ],
      "id": "IiMThsYeDa2O"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBY0b6zxI0r9"
      },
      "source": [
        "1. Executing the below code which will provide you with an authentication link\n",
        "2. Open the link\n",
        "3. Choose the Google account whose Drive you want to mount\n",
        "4. Allow Google Drive Stream access to your Google Account\n",
        "5. Copy the code displayed, paste it in the text box as shown below, and press Enter\n",
        "![](https://i1.wp.com/neptune.ai/wp-content/uploads/colab-code-copy.png?resize=512%2C102&ssl=1)\n",
        "\n",
        "Finish!\n",
        "\n",
        "---\n",
        "\n",
        "執行此段後點選出現的連結，允許授權後，複製授權碼，貼在空格中後按下ENTER，即完成與雲端硬碟連結。"
      ],
      "id": "jBY0b6zxI0r9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCXepUIVe5iJ",
        "outputId": "cb00b6df-81f6-4033-f7ad-29cd2879f9eb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "cCXepUIVe5iJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqO8DiB6VRQZ"
      },
      "source": [
        "## Unzip Data\n",
        "\n",
        "Unzip `flickr8k.zip`, there are 2 folders and 1 txt.\n",
        "\n",
        "- `images/`: contains images for training.\n",
        "- `test_examples/`: contains images for testing.\n",
        "- `captions.txt`: the file_names and their captions.\n",
        "\n",
        "There are **8091 images in images.**  \n",
        "There are **5 images in test_examples.**   \n",
        "\n",
        "---\n",
        "\n",
        "解壓縮 `flickr8k.zip` 後可以發現裡面有兩個資料夾和三個csv檔。\n",
        "\n",
        "+ `images` : 訓練用圖片。\n",
        "+ `test_examples` : 測試結果用的圖片。\n",
        "+ `captions.txt` : 圖片檔名及對應的敘述。\n",
        "\n",
        "其中`images`的圖片  8091 張，`test_examples` 的圖片 5 張\n",
        "\n",
        "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
      ],
      "id": "cqO8DiB6VRQZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGCZQxZSfONu"
      },
      "outputs": [],
      "source": [
        "!unzip -qq ./drive/My\\ Drive/flickr8k.zip"
      ],
      "id": "aGCZQxZSfONu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9c9d55e",
        "outputId": "b2e451a9-b07c-4db6-ff2f-76fc3a8aecbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os  # when loading file paths\n",
        "import pandas as pd  # for lookup in annotation file\n",
        "import spacy  # for tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load img\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# We want to convert text -> numerical values\n",
        "# 1. We need a Vocabulary mapping each word to a index\n",
        "# 2. We need to setup a Pytorch dataset to load the data\n",
        "# 3. Setup padding of every batch (all examples should be of same seq_len and setup dataloader)\n",
        "# Note that loading the image is very easy compared to the text!"
      ],
      "id": "b9c9d55e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ba6f033"
      },
      "source": [
        "## Step1 Data Processing & DataLoader"
      ],
      "id": "3ba6f033"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "43143cae"
      },
      "outputs": [],
      "source": [
        "# Download with: python -m spacy download en \n",
        "# the tokenize\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ],
      "id": "43143cae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "704efc54"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self, freq_threshold):\n",
        "    self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} #some index to string\n",
        "    self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3} #some string to index\n",
        "    self.freq_threshold = freq_threshold\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.itos)\n",
        "\n",
        "  @staticmethod\n",
        "  def tokenizer_eng(text):\n",
        "    return [tok.text.lower() for tok in spacy_eng.tokenizer(text)] #lowercase all the words\n",
        "    # \"I like Deep Learning\" >>> [\"i\", \"like\", \"deep\", \"learning\"]\n",
        "\n",
        "  def build_vocabulary(self, sentence_list):\n",
        "    frequencies = {}\n",
        "    idx = 4 # 0-3 have already been used\n",
        "\n",
        "    for sentence in sentence_list:\n",
        "        for word in self.tokenizer_eng(sentence):\n",
        "            if word not in frequencies:\n",
        "                frequencies[word] = 1\n",
        "\n",
        "            else:\n",
        "                frequencies[word] += 1\n",
        "\n",
        "            if frequencies[word] == self.freq_threshold:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "  def numericalize(self, text):\n",
        "    tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "    return [\n",
        "        self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "        for token in tokenized_text\n",
        "    ]"
      ],
      "id": "704efc54"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e9925a4"
      },
      "outputs": [],
      "source": [
        "class FlickrDataset(Dataset):\n",
        "  def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "    self.root_dir = root_dir\n",
        "    self.df = pd.read_csv(captions_file)\n",
        "    self.transform = transform\n",
        "\n",
        "    # Get img, caption columns\n",
        "    self.imgs = self.df[\"image\"]\n",
        "    self.captions = self.df[\"caption\"]\n",
        "\n",
        "    # Initialize vocabulary and build vocab\n",
        "    self.vocab = Vocabulary(freq_threshold)\n",
        "    self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.df)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    caption = self.captions[index]\n",
        "    img_id = self.imgs[index]\n",
        "    img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "    if self.transform is not None:\n",
        "        img = self.transform(img)\n",
        "\n",
        "    numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "    numericalized_caption += self.vocab.numericalize(caption)\n",
        "    numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "    return img, torch.tensor(numericalized_caption)"
      ],
      "id": "8e9925a4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "079359b1"
      },
      "outputs": [],
      "source": [
        "class MyCollate:\n",
        "  def __init__(self, pad_idx):\n",
        "      self.pad_idx = pad_idx\n",
        "\n",
        "  def __call__(self, batch):\n",
        "    imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "    imgs = torch.cat(imgs, dim=0)\n",
        "    targets = [item[1] for item in batch]\n",
        "    targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n",
        "\n",
        "    return imgs, targets"
      ],
      "id": "079359b1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bc25bd4c"
      },
      "outputs": [],
      "source": [
        "def get_loader(\n",
        "  root_folder,\n",
        "  annotation_file,\n",
        "  transform,\n",
        "  batch_size=32,\n",
        "  num_workers=8,\n",
        "  shuffle=True,\n",
        "  pin_memory=True,\n",
        "):\n",
        "  dataset = FlickrDataset(root_folder, annotation_file, transform=transform)\n",
        "\n",
        "  pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "  loader = DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      num_workers=num_workers,\n",
        "      shuffle=shuffle,\n",
        "      pin_memory=pin_memory,\n",
        "      collate_fn=MyCollate(pad_idx=pad_idx),\n",
        "  )\n",
        "\n",
        "  return loader, dataset\n"
      ],
      "id": "bc25bd4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y7KF6mbEksz"
      },
      "source": [
        "## Your Turn: Design Your Augmentation(5 points)"
      ],
      "id": "8Y7KF6mbEksz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tQGLuVWnA-b"
      },
      "source": [
        "### Data augmentation \n",
        "\n",
        "Data augmentation are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
        "\n",
        "PyTorch use `torchvision.transforms` to do data augmentation.\n",
        "[You can see all function here.](https://pytorch.org/vision/stable/transforms.html)\n",
        "\n"
      ],
      "id": "5tQGLuVWnA-b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "af963390"
      },
      "outputs": [],
      "source": [
        "######################################################################################\n",
        "#  TODO: use transforms.xxx method to do some data augmentation       #\n",
        "#  This one is for training, find the composition to get better result  #\n",
        "######################################################################################\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize((356, 356)),\n",
        "  transforms.CenterCrop((299,299)),\n",
        "  #transforms.Resize((256, 256)),\n",
        "  #transforms.CenterCrop((224,224)),\n",
        "  transforms.RandomHorizontalFlip(p=0.5),\n",
        "  transforms.ToTensor(),\n",
        "  #transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5]),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])  \n",
        "########################################################################\n",
        "#             End of your code              #\n",
        "########################################################################"
      ],
      "id": "af963390"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7e9e313",
        "outputId": "4a2dcb73-cedf-40d4-8668-3bc769c8b0a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the imgs :  torch.Size([32, 3, 299, 299])\n",
            "Shape of the captions :  torch.Size([19, 32])\n"
          ]
        }
      ],
      "source": [
        "train_loader, dataset = get_loader(\n",
        "  root_folder=\"flickr8k/images/\",\n",
        "  annotation_file=\"flickr8k/captions.txt\",\n",
        "  transform=transform,\n",
        "  num_workers = 0,\n",
        ")\n",
        "\n",
        "########################\n",
        "dataL = iter(train_loader)\n",
        "imgs, captions = next(dataL)\n",
        "print(\"Shape of the imgs : \", imgs.shape)\n",
        "print(\"Shape of the captions : \", captions.shape)"
      ],
      "id": "e7e9e313"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DcJACYO-PB8F"
      },
      "source": [
        "# Build Your Model\n"
      ],
      "id": "DcJACYO-PB8F"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f53c6721"
      },
      "source": [
        "<a id='step3'></a>\n",
        "## Step 2(Your Turn): Implement the CNN Encoder(15 points)\n",
        "\n",
        "\n",
        "You can use pre-trained models like ResNet-50 architecture (with the final fully-connected layer removed) to extract features from a batch of pre-processed images.  The output is then flattened to a vector, before being passed through a `Linear` layer to transform the feature vector to have the same size as the word embedding.\n",
        "\n",
        "In the code cell below, `output` should be a PyTorch tensor with size `[batch_size, embed_size]`."
      ],
      "id": "f53c6721"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a21c815a"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import statistics\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size, train_CNN=False):\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        \n",
        "        ########################################################################\n",
        "        #     TODO: Design your CNN model structure                            #\n",
        "        ########################################################################        \n",
        "        self.train_CNN = train_CNN\n",
        "        self.inception = models.inception_v3(pretrained = True)\n",
        "        self.inception.aux_logits = False\n",
        "        self.inception.fc = nn.Linear(self.inception.fc.in_features, embed_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, images):\n",
        "        \n",
        "        ########################################################################\n",
        "        #     TODO: Design your CNN forward method                             #\n",
        "        ########################################################################\n",
        "               \n",
        "        features = self.inception(images)\n",
        "\n",
        "        for name, param in self.inception.named_parameters():\n",
        "          if \"fc.weight\" in name or \"fc.bias\" in name:\n",
        "            param.requires_grad = True\n",
        "          else:\n",
        "            param.requires_grad = self.train_CNN\n",
        "\n",
        "        return  self.dropout(self.relu((features)))   \n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "        "
      ],
      "id": "a21c815a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd01de49"
      },
      "source": [
        "<a id='step4'></a>\n",
        "## Step 3(Your Turn):  Implement the RNN Decoder(15 points)\n",
        "\n",
        "In this part, you should design your RNN Decoder.\n",
        "\n",
        "You might want to use some layers such as `torch.nn.LSTM` or `orch.nn.Embedding`, you can find some informatin in the chapter #7 slide.\n",
        "\n",
        "In the code cell below, `outputs` should be a PyTorch tensor with size `[captions.shape[0], batch_size, vocab_size]`.  "
      ],
      "id": "cd01de49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aa8badd4"
      },
      "outputs": [],
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        #     TODO: Design your RNN model structure         #\n",
        "        ########################################################################\n",
        "        \n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        \n",
        "        ########################################################################\n",
        "        #     TODO: Design your RNN forward method         #\n",
        "        ########################################################################      \n",
        "        embeddings = self.dropout(self.embed(captions))\n",
        "        embeddings = torch.cat((features.unsqueeze(0), embeddings), dim = 0)\n",
        "        hiddens, _ = self.lstm(embeddings)\n",
        "        outputs = self.linear(hiddens)\n",
        "        \n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "        return outputs"
      ],
      "id": "aa8badd4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c88984b0"
      },
      "source": [
        "<a id='step4'></a>\n",
        "## Step 4(Your Turn): Combine the CNN Encoder with the RNN Decoder(10 point)\n",
        "\n",
        "In this part, you should combline the encoder and the decoder you designed above. To do that, you simply have to take the outputs of your encoder and the captions and send them into the decoder.\n",
        "\n",
        "\n",
        "\n"
      ],
      "id": "c88984b0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04ea3ffd"
      },
      "outputs": [],
      "source": [
        "class CNNtoRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers):\n",
        "        super(CNNtoRNN, self).__init__()\n",
        "\n",
        "        ########################################################################\n",
        "        #     TODO: Design your model forward method         #\n",
        "        ########################################################################\n",
        "\n",
        "        self.encoderCNN = EncoderCNN(embed_size)\n",
        "        self.decoderRNN = DecoderRNN(embed_size, hidden_size, vocab_size, num_layers)\n",
        "\n",
        "\n",
        "        \n",
        "\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "        \n",
        "    def forward(self, images, captions):\n",
        "        ########################################################################\n",
        "        #     TODO: Design your model forward method         #\n",
        "        ########################################################################\n",
        "        features = self.encoderCNN(images)\n",
        "        outputs = self.decoderRNN(features, captions)\n",
        "\n",
        "        ########################################################################\n",
        "        #                           End of your code                           #\n",
        "        ########################################################################\n",
        "        return outputs\n",
        "\n",
        "    def caption_image(self, image, vocabulary, max_length=50):\n",
        "        result_caption = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            x = self.encoderCNN(image).unsqueeze(0)\n",
        "            states = None\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                hiddens, states = self.decoderRNN.lstm(x, states)\n",
        "                output = self.decoderRNN.linear(hiddens.squeeze(0))\n",
        "                predicted = output.argmax(1)\n",
        "                result_caption.append(predicted.item())\n",
        "                x = self.decoderRNN.embed(predicted).unsqueeze(0)\n",
        "\n",
        "                if vocabulary.itos[predicted.item()] == \"<EOS>\":\n",
        "                    break\n",
        "\n",
        "        return [vocabulary.itos[idx] for idx in result_caption]"
      ],
      "id": "04ea3ffd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2QkDsWHPKMo"
      },
      "source": [
        "#Traning Process"
      ],
      "id": "b2QkDsWHPKMo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4114ba79"
      },
      "source": [
        "<a id='step5'></a>\n",
        "## Step 5: Training Settings\n",
        "\n",
        "You can modify some hyperparameters to improve the performance of you model."
      ],
      "id": "4114ba79"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "6b019709e8bc496b822c10b4d8493f29",
            "33961fe80c764431aa0cf3ad5e4c89d0",
            "984b3ae680354325a252953bc8664fa3",
            "41fe2bc60dba4d20b053e4220e886a65",
            "c24b07616f4a4005bdadd6158481c1a5",
            "d2565a3a9dfa4ba3a71e29c3141816f0",
            "640fc14b0c834e8e8f808d35c0724d53",
            "a0f09185581d4de79cea8ab7fe2b0e00",
            "8c8eef1781624758bbe098b301e572a3",
            "4f7eca8fef244179816bacbdfef15142",
            "5e980f92762844149f2986a8343c9697"
          ]
        },
        "id": "9c574636",
        "outputId": "43405243-4590-4836-f4f8-f6185927b4b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=Inception_V3_Weights.IMAGENET1K_V1`. You can also use `weights=Inception_V3_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/104M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6b019709e8bc496b822c10b4d8493f29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CNNtoRNN(\n",
              "  (encoderCNN): EncoderCNN(\n",
              "    (inception): Inception3(\n",
              "      (Conv2d_1a_3x3): BasicConv2d(\n",
              "        (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (Conv2d_2a_3x3): BasicConv2d(\n",
              "        (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (Conv2d_2b_3x3): BasicConv2d(\n",
              "        (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (maxpool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (Conv2d_3b_1x1): BasicConv2d(\n",
              "        (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (Conv2d_4a_3x3): BasicConv2d(\n",
              "        (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
              "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "      (maxpool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "      (Mixed_5b): InceptionA(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_2): BasicConv2d(\n",
              "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_5c): InceptionA(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_1): BasicConv2d(\n",
              "          (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_2): BasicConv2d(\n",
              "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_5d): InceptionA(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_1): BasicConv2d(\n",
              "          (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch5x5_2): BasicConv2d(\n",
              "          (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6a): InceptionB(\n",
              "        (branch3x3): BasicConv2d(\n",
              "          (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6b): InceptionC(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_3): BasicConv2d(\n",
              "          (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_4): BasicConv2d(\n",
              "          (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_5): BasicConv2d(\n",
              "          (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6c): InceptionC(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_2): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_3): BasicConv2d(\n",
              "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_4): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_5): BasicConv2d(\n",
              "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6d): InceptionC(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_2): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_3): BasicConv2d(\n",
              "          (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_4): BasicConv2d(\n",
              "          (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_5): BasicConv2d(\n",
              "          (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_6e): InceptionC(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7_3): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_3): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_4): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7dbl_5): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (AuxLogits): InceptionAux(\n",
              "        (conv0): BasicConv2d(\n",
              "          (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (conv1): BasicConv2d(\n",
              "          (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (fc): Linear(in_features=768, out_features=1000, bias=True)\n",
              "      )\n",
              "      (Mixed_7a): InceptionD(\n",
              "        (branch3x3_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7x3_1): BasicConv2d(\n",
              "          (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7x3_2): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7x3_3): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch7x7x3_4): BasicConv2d(\n",
              "          (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_7b): InceptionE(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_1): BasicConv2d(\n",
              "          (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2a): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2b): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3a): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3b): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (Mixed_7c): InceptionE(\n",
              "        (branch1x1): BasicConv2d(\n",
              "          (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_1): BasicConv2d(\n",
              "          (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2a): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3_2b): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_1): BasicConv2d(\n",
              "          (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_2): BasicConv2d(\n",
              "          (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3a): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch3x3dbl_3b): BasicConv2d(\n",
              "          (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
              "          (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "        (branch_pool): BasicConv2d(\n",
              "          (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "      (dropout): Dropout(p=0.5, inplace=False)\n",
              "      (fc): Linear(in_features=2048, out_features=256, bias=True)\n",
              "    )\n",
              "    (relu): ReLU()\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoderRNN): DecoderRNN(\n",
              "    (embed): Embedding(2994, 256)\n",
              "    (lstm): LSTM(256, 256)\n",
              "    (linear): Linear(in_features=256, out_features=2994, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "train_CNN = False\n",
        "\n",
        "vocab_size = len(dataset.vocab)\n",
        "################## Hyperparameters ##################\n",
        "embed_size = 256\n",
        "hidden_size = 256\n",
        "vocab_size = len(dataset.vocab)\n",
        "num_layers = 1 #number of LSTM layers\n",
        "learning_rate = 1e-3\n",
        "\n",
        "#Each epoch would probably take upto two hours to train on Colab, so start early.\n",
        "num_epochs = 2\n",
        "\n",
        "######################################################\n",
        "\n",
        "# initialize model, loss etc\n",
        "model = CNNtoRNN(embed_size, hidden_size, vocab_size, num_layers).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "model.train()"
      ],
      "id": "9c574636"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48e28c91"
      },
      "source": [
        "<a id='step6'></a>\n",
        "## Step 6: Train Your Model\n",
        "\n",
        "Each epoch would probably take over an hour to train on Colab, so start early."
      ],
      "id": "48e28c91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d32afd1f",
        "outputId": "07acb576-d700-4400-e4ad-d5e0d1774ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs [1/2] ------- Loss [3.4112] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 86%|████████▌ | 1083/1265 [3:06:52<31:05, 10.25s/it]"
          ]
        }
      ],
      "source": [
        "for epoch in range(num_epochs):\n",
        "            \n",
        "  for idx, (imgs, captions) in tqdm(\n",
        "      enumerate(train_loader), total=len(train_loader), leave=False\n",
        "  ):\n",
        "                 \n",
        "    imgs = imgs.to(device)\n",
        "    captions = captions.to(device)\n",
        "\n",
        "    outputs = model(imgs, captions[:-1])\n",
        "    loss = criterion(\n",
        "        outputs.reshape(-1, outputs.shape[2]), captions.reshape(-1)\n",
        "    )\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward(loss)\n",
        "    optimizer.step()\n",
        "        \n",
        "      \n",
        "  stat_vals = 'Epochs [%d/%d] ------- Loss [%.4f] ' %( epoch + 1 ,num_epochs,loss.item() )\n",
        "  print(stat_vals)\n",
        "  sys.stdout.flush()"
      ],
      "id": "d32afd1f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzZZRyKKPQap"
      },
      "source": [
        "#Check The Results"
      ],
      "id": "mzZZRyKKPQap"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcc7f8b"
      },
      "source": [
        "<a id='step7'></a>\n",
        "## Step 7: Simple Test\n",
        "\n",
        "This part is for the testing. After training, run the codes below to see some example captioning output. Note that since we only train for few epochs, the results might not be good. If you want to improve the result, you can try to train for more epachs, though it might takes a couple of hours."
      ],
      "id": "6bcc7f8b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71681e9d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_examples(model, device, dataset):\n",
        "  transform = transforms.Compose(\n",
        "      [\n",
        "          transforms.Resize((256, 256)),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  model.eval()\n",
        "  img1 = Image.open(\"flickr8k/test_examples/dog.jpg\")\n",
        "  test_img1 = transform(img1.convert(\"RGB\")).unsqueeze(0)\n",
        "  print(\"Example 1 CORRECT: Dog on a beach by the ocean\")\n",
        "  print(\n",
        "      \"Example 1 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img1.to(device), dataset.vocab))\n",
        "  )\n",
        "  plt.imshow(img1)\n",
        "  plt.show()\n",
        "  \n",
        "  img2 = Image.open(\"flickr8k/test_examples/child.jpg\")\n",
        "  test_img2 = transform(img2.convert(\"RGB\")).unsqueeze(0)\n",
        "  print(\"Example 2 CORRECT: Child holding red frisbee outdoors\")\n",
        "  print(\n",
        "      \"Example 2 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img2.to(device), dataset.vocab))\n",
        "  )\n",
        "  plt.imshow(img2)\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "  img3 = Image.open(\"flickr8k/test_examples/bus.png\")\n",
        "  test_img3 = transform(img3.convert(\"RGB\")).unsqueeze(0)\n",
        "  print(\"Example 3 CORRECT: Bus driving by parked cars\")\n",
        "  print(\n",
        "      \"Example 3 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img3.to(device), dataset.vocab))\n",
        "  )\n",
        "  plt.imshow(img3)\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "  img4 = Image.open(\"flickr8k/test_examples/boat.png\")\n",
        "  test_img4 = transform(img4.convert(\"RGB\")).unsqueeze(0)\n",
        "  print(\"Example 4 CORRECT: A small boat in the ocean\")\n",
        "  print(\n",
        "      \"Example 4 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img4.to(device), dataset.vocab))\n",
        "  )\n",
        "  plt.imshow(img4)\n",
        "  plt.show()\n",
        "  \n",
        "  \n",
        "  img5 = Image.open(\"flickr8k/test_examples/horse.png\")\n",
        "  test_img5 = transform(img5.convert(\"RGB\")).unsqueeze(0)\n",
        "  print(\"Example 5 CORRECT: A cowboy riding a horse in the desert\")\n",
        "  print(\n",
        "      \"Example 5 OUTPUT: \"\n",
        "      + \" \".join(model.caption_image(test_img5.to(device), dataset.vocab))\n",
        "  )\n",
        "  plt.imshow(img5)\n",
        "  plt.show()\n",
        "  \n",
        "  model.train()\n"
      ],
      "id": "71681e9d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6b700c3"
      },
      "outputs": [],
      "source": [
        "# Run the line below to see a couple of test cases\n",
        "print_examples(model, device, dataset)"
      ],
      "id": "b6b700c3"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6b019709e8bc496b822c10b4d8493f29": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_33961fe80c764431aa0cf3ad5e4c89d0",
              "IPY_MODEL_984b3ae680354325a252953bc8664fa3",
              "IPY_MODEL_41fe2bc60dba4d20b053e4220e886a65"
            ],
            "layout": "IPY_MODEL_c24b07616f4a4005bdadd6158481c1a5"
          }
        },
        "33961fe80c764431aa0cf3ad5e4c89d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2565a3a9dfa4ba3a71e29c3141816f0",
            "placeholder": "​",
            "style": "IPY_MODEL_640fc14b0c834e8e8f808d35c0724d53",
            "value": "100%"
          }
        },
        "984b3ae680354325a252953bc8664fa3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a0f09185581d4de79cea8ab7fe2b0e00",
            "max": 108949747,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c8eef1781624758bbe098b301e572a3",
            "value": 108949747
          }
        },
        "41fe2bc60dba4d20b053e4220e886a65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f7eca8fef244179816bacbdfef15142",
            "placeholder": "​",
            "style": "IPY_MODEL_5e980f92762844149f2986a8343c9697",
            "value": " 104M/104M [00:00&lt;00:00, 226MB/s]"
          }
        },
        "c24b07616f4a4005bdadd6158481c1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2565a3a9dfa4ba3a71e29c3141816f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "640fc14b0c834e8e8f808d35c0724d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a0f09185581d4de79cea8ab7fe2b0e00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c8eef1781624758bbe098b301e572a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f7eca8fef244179816bacbdfef15142": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e980f92762844149f2986a8343c9697": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}