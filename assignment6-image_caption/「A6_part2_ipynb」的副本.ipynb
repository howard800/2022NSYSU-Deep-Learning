{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "mYkZGeyR2au4",
      "metadata": {
        "id": "mYkZGeyR2au4"
      },
      "source": [
        "# MIS 583 Assignment 6 Part 2: Image Captioning with Attention\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YOPpE4vN2jc8",
      "metadata": {
        "id": "YOPpE4vN2jc8"
      },
      "source": [
        "Before we start, please put your name and ID in following format: <br>\n",
        "NAME, ?000000000   e.g. 陳琨翔, M094020003\n",
        "\n",
        "**Your Answer:**   \n",
        "Hi I'm 王渙鈞, M114020052"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B_MoQiztpxcK",
      "metadata": {
        "id": "B_MoQiztpxcK"
      },
      "source": [
        "## Image Captioning with Attention\n",
        "\n",
        "The use of Attention networks is widespread in deep learning, and with good reason. This is a way for a model to choose only those parts of the encoding that it thinks is relevant to the task at hand. The same mechanism you see employed here can be used in any model where the Encoder's output has multiple points in space or time. In image captioning, you consider some pixels more important than others. In sequence to sequence tasks like machine translation, you consider some words more important than others.\n",
        "\n",
        "In the assignment, you will design and train a Image captioning network with Attention mechanism from scratch to process the input image, then output a sequence that describe the image. \n",
        "\n",
        "**Note that you are free to use pre-trained models like ResNet or LSTM as your backbone structure.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "giUId1Naqacs",
      "metadata": {
        "id": "giUId1Naqacs",
        "tags": []
      },
      "source": [
        "##  Versions of used packages\n",
        "\n",
        "We will check PyTorch version to make sure everything work properly.\n",
        "\n",
        "I use `python 3.8.11`, `torch==1.8.2` and `torchvision==0.9.2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vuw-gNvjqcYe",
      "metadata": {
        "id": "Vuw-gNvjqcYe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c50e0d4d-a847-4bbb-8b3a-ad86669fcbb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python 3.8.16 (default, Dec  7 2022, 01:12:13) \n",
            "torch 1.13.0+cu116\n",
            "torchvision 0.14.0+cu116\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "print('python', sys.version.split('\\n')[0])\n",
        "print('torch', torch.__version__)\n",
        "print('torchvision', torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_FydjP4MTWuL",
      "metadata": {
        "id": "_FydjP4MTWuL"
      },
      "source": [
        "## Error handling\n",
        "\n",
        "**RuntimeError: CUDA out of memory...**\n",
        "> 發生原因可能為讀取的 batch 過大或是記憶體未釋放乾淨。若縮小 batch size 後仍出現錯誤請按照以下步驟重新載入 colab。\n",
        "> The reason of this error is over-sized batch_size or unreleased memory. If changing the batch_size smaller still cause this error. Please reload colab follow below instructions.\n",
        "\n",
        "1. Click 「Runtime」\n",
        "2. Click 「Factor reset runtime」\n",
        "3. Click 「Reconnect」\n",
        "4. Reload all chunk\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OhdbdJOsrbxL",
      "metadata": {
        "id": "OhdbdJOsrbxL"
      },
      "source": [
        "# Prepare Data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nPoSgD83teTQ",
      "metadata": {
        "id": "nPoSgD83teTQ"
      },
      "source": [
        "For reference, we use [Flickr8k](https://www.kaggle.com/dataset/e1cd22253a9b23b073794872bf565648ddbe4f17e7fa9e74766ad3707141adeb) dataset.\n",
        "This dataset is collected by Alexander Mamaev.\n",
        "\n",
        "**Abstrct**  \n",
        "A new benchmark collection for sentence-based image description and search, consisting of 8,091 images that are each paired with five different captions which provide clear descriptions of the salient entities and events. The images were chosen from six different Flickr groups, and tend not to contain any well-known people or locations, but were manually selected to depict a variety of scenes and situations\n",
        "\n",
        "Photos are not reduced to a single size, they have different proportions!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IiMThsYeDa2O",
      "metadata": {
        "id": "IiMThsYeDa2O"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## How to Get Data\n",
        "\n",
        "Please open the file `flickr8k.zip`, creat shortcut to your Google Drive.\n",
        "\n",
        "1. open [LINK of Google Drive](https://drive.google.com/file/d/1awvS-E5IPMJgu6kiRjZbiwyrmx6Cl6VO/view?usp=sharing)\n",
        "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
        "3. Select the location where you want to place the shortcut.\n",
        "4. Click Add shortcut.\n",
        "\n",
        "After above procedures, we have a shortcut of zip file of dataset.  \n",
        "We can access this in colab after granting the permission of Google Drive.\n",
        "\n",
        "---\n",
        "\n",
        "請先到共用雲端硬碟將檔案 `flickr8k.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/1awvS-E5IPMJgu6kiRjZbiwyrmx6Cl6VO/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jBY0b6zxI0r9",
      "metadata": {
        "id": "jBY0b6zxI0r9"
      },
      "source": [
        "1. Executing the below code which will provide you with an authentication link\n",
        "2. Open the link\n",
        "3. Choose the Google account whose Drive you want to mount\n",
        "4. Allow Google Drive Stream access to your Google Account\n",
        "5. Copy the code displayed, paste it in the text box as shown below, and press Enter\n",
        "![](https://i1.wp.com/neptune.ai/wp-content/uploads/colab-code-copy.png?resize=512%2C102&ssl=1)\n",
        "\n",
        "Finish!\n",
        "\n",
        "---\n",
        "\n",
        "執行此段後點選出現的連結，允許授權後，複製授權碼，貼在空格中後按下ENTER，即完成與雲端硬碟連結。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cCXepUIVe5iJ",
      "metadata": {
        "id": "cCXepUIVe5iJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2b7fdbe-f853-4076-d529-9ac70cc54836"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cqO8DiB6VRQZ",
      "metadata": {
        "id": "cqO8DiB6VRQZ"
      },
      "source": [
        "## Unzip Data\n",
        "\n",
        "Unzip `flickr8k.zip`, there are 2 folders and 1 txt.\n",
        "\n",
        "- `images/`: contains images for training.\n",
        "- `test_examples/`: contains images for testing.\n",
        "- `captions.txt`: the file_names and their captions.\n",
        "\n",
        "There are **8091 images in images.**  \n",
        "There are **5 images in test_examples.**   \n",
        "\n",
        "---\n",
        "\n",
        "解壓縮 `flickr8k.zip` 後可以發現裡面有兩個資料夾和三個csv檔。\n",
        "\n",
        "+ `images` : 訓練用圖片。\n",
        "+ `test_examples` : 測試結果用的圖片。\n",
        "+ `captions.txt` : 圖片檔名及對應的敘述。\n",
        "\n",
        "其中`images`的圖片  8091 張，`test_examples` 的圖片 5 張\n",
        "\n",
        "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aGCZQxZSfONu",
      "metadata": {
        "id": "aGCZQxZSfONu"
      },
      "outputs": [],
      "source": [
        "!unzip -qq ./drive/My\\ Drive/flickr8k.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6b1f6d",
      "metadata": {
        "id": "cf6b1f6d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd2acbd0-d740-4d04-8c7f-2ce7278f87fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML\n",
            "  warnings.warn(\"Can't initialize NVML\")\n"
          ]
        }
      ],
      "source": [
        "import os  # when loading file paths\n",
        "import pandas as pd  # for lookup in annotation file\n",
        "import spacy  # for tokenizer\n",
        "from torch.nn.utils.rnn import pad_sequence  # pad batch\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image  # Load img\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn.functional as F\n",
        "import torchvision.models as models\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ba6f033",
      "metadata": {
        "id": "3ba6f033"
      },
      "source": [
        "## Step1 Data Processing & DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4aac1cb",
      "metadata": {
        "id": "f4aac1cb"
      },
      "outputs": [],
      "source": [
        "# Download with: python -m spacy download en \n",
        "# the tokenize\n",
        "spacy_eng = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c427c939",
      "metadata": {
        "id": "c427c939"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self, freq_threshold):\n",
        "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"} #some index to string\n",
        "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3} #some string to index\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenizer_eng(text):\n",
        "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)] #lowercase all the words\n",
        "        # \"I like Deep Learning\" >>> [\"i\", \"like\", \"deep\", \"learning\"]\n",
        "    \n",
        "    def build_vocabulary(self, sentence_list):\n",
        "        frequencies = {}\n",
        "        idx = 4 # 0-3 have already been used\n",
        "\n",
        "        for sentence in sentence_list:\n",
        "            for word in self.tokenizer_eng(sentence):\n",
        "                if word not in frequencies:\n",
        "                    frequencies[word] = 1\n",
        "\n",
        "                else:\n",
        "                    frequencies[word] += 1\n",
        "\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        tokenized_text = self.tokenizer_eng(text)\n",
        "\n",
        "        return [\n",
        "            self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"]\n",
        "            for token in tokenized_text\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bb72f2b",
      "metadata": {
        "id": "4bb72f2b"
      },
      "outputs": [],
      "source": [
        "class FlickrDataset(Dataset):\n",
        "    def __init__(self, root_dir, captions_file, transform=None, freq_threshold=5):\n",
        "        self.root_dir = root_dir\n",
        "        self.df = pd.read_csv(captions_file)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Get img, caption columns\n",
        "        self.imgs = self.df[\"image\"]\n",
        "        self.captions = self.df[\"caption\"]\n",
        "\n",
        "        # Initialize vocabulary and build vocab\n",
        "        self.vocab = Vocabulary(freq_threshold)\n",
        "        self.vocab.build_vocabulary(self.captions.tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        caption = self.captions[index]\n",
        "        img_id = self.imgs[index]\n",
        "        img = Image.open(os.path.join(self.root_dir, img_id)).convert(\"RGB\")\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
        "        numericalized_caption += self.vocab.numericalize(caption)\n",
        "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
        "\n",
        "        return img, torch.tensor(numericalized_caption)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c803d37",
      "metadata": {
        "id": "9c803d37"
      },
      "outputs": [],
      "source": [
        "class MyCollate:\n",
        "    def __init__(self, pad_idx):\n",
        "        self.pad_idx = pad_idx\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        imgs = [item[0].unsqueeze(0) for item in batch]\n",
        "        imgs = torch.cat(imgs, dim = 0)\n",
        "        targets = [item[1] for item in batch]\n",
        "        targets = pad_sequence(targets, batch_first = False, padding_value = self.pad_idx)\n",
        "\n",
        "        return imgs, targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b821421",
      "metadata": {
        "id": "0b821421"
      },
      "outputs": [],
      "source": [
        "def get_loader(\n",
        "    root_folder,\n",
        "    annotation_file,\n",
        "    transform,\n",
        "    batch_size = 32,\n",
        "    num_workers = 8,\n",
        "    shuffle = True,\n",
        "    pin_memory = True,\n",
        "):\n",
        "    dataset = FlickrDataset(root_folder, annotation_file, transform = transform)\n",
        "\n",
        "    pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset = dataset,\n",
        "        batch_size = batch_size,\n",
        "        num_workers = num_workers,\n",
        "        shuffle = shuffle,\n",
        "        pin_memory = pin_memory,\n",
        "        collate_fn = MyCollate(pad_idx = pad_idx),\n",
        "    )\n",
        "\n",
        "    return loader, dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8Y7KF6mbEksz",
      "metadata": {
        "id": "8Y7KF6mbEksz"
      },
      "source": [
        "## Your Turn: Design Your Augmentation(5 points)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5tQGLuVWnA-b",
      "metadata": {
        "id": "5tQGLuVWnA-b"
      },
      "source": [
        "### Data augmentation \n",
        "\n",
        "Data augmentation are techniques used to increase the amount of data by adding slightly modified copies of already existing data or newly created synthetic data from existing data.\n",
        "\n",
        "PyTorch use `torchvision.transforms` to do data augmentation.\n",
        "[You can see all function here.](https://pytorch.org/vision/stable/transforms.html)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c650824c",
      "metadata": {
        "id": "c650824c"
      },
      "outputs": [],
      "source": [
        "######################################################################################\n",
        "#  TODO: use transforms.xxx method to do some data augmentation       #\n",
        "#  This one is for training, find the composition to get better result  #\n",
        "######################################################################################\n",
        "transform = transforms.Compose([\n",
        "  transforms.Resize((356, 356)),\n",
        "  transforms.CenterCrop((256,256)),\n",
        "  #transforms.Resize((256, 256)),\n",
        "  #transforms.CenterCrop((224,224)),\n",
        "  transforms.RandomHorizontalFlip(p=0.5),\n",
        "  transforms.ToTensor(),\n",
        "  #transforms.Normalize(mean = [0.5, 0.5, 0.5], std = [0.5, 0.5, 0.5]),\n",
        "  transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])  \n",
        "########################################################################\n",
        "#             End of your code              #\n",
        "########################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4baee8d",
      "metadata": {
        "id": "b4baee8d",
        "outputId": "065864e3-3ac2-4442-cf0e-c43550f606c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the imgs :  torch.Size([32, 3, 256, 256])\n",
            "Shape of the captions :  torch.Size([22, 32])\n"
          ]
        }
      ],
      "source": [
        "train_loader, dataset = get_loader(\n",
        "  root_folder=\"flickr8k/images/\",\n",
        "  annotation_file=\"flickr8k/captions.txt\",\n",
        "  transform=transform,\n",
        "  num_workers=0,\n",
        ")\n",
        "\n",
        "########################\n",
        "dataL = iter(train_loader)\n",
        "imgs, captions = next(dataL)\n",
        "print(\"Shape of the imgs : \", imgs.shape)\n",
        "print(\"Shape of the captions : \", captions.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DcJACYO-PB8F",
      "metadata": {
        "id": "DcJACYO-PB8F"
      },
      "source": [
        "# Build Your Model Structure\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f53c6721",
      "metadata": {
        "id": "f53c6721"
      },
      "source": [
        "<a id='step3'></a>\n",
        "## Step 2(Your Turn): Implement the CNN Encoder(10 points)\n",
        "\n",
        "\n",
        "You can use pre-trained models like ResNet-50 architecture (with the final fully-connected layer removed) to extract features from a batch of pre-processed images. \n",
        "\n",
        "In the code cell below, `features` should be a PyTorch tensor with size `[batch_size, num_pixels, encoder_dim]`.\n",
        "\n",
        "Ex.if your pretrained model is resnet50, your output `features` should be the size of `[batch_size, 49, 2048]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2b3d8494",
      "metadata": {
        "id": "2b3d8494"
      },
      "outputs": [],
      "source": [
        "class EncoderAttentionCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderAttentionCNN, self).__init__()\n",
        "        ######################################################################################\n",
        "        #  TODO: TODO: Design your CNN model structure                 #\n",
        "        ######################################################################################\n",
        "        model = torch.hub.load('pytorch/vision:v0.10.0', 'resnext101_32x8d', pretrained=True)\n",
        "        self.pretrained = nn.Sequential(*(list(model.children())[:-2]))\n",
        "        \n",
        "        for param in list(self.pretrained.children())[:-3]:\n",
        "             param.requires_grad_(False)\n",
        "\n",
        "        ########################################################################\n",
        "        #             End of your code              #\n",
        "        ########################################################################\n",
        "\n",
        "    def forward(self, images):\n",
        "\n",
        "        ######################################################################################\n",
        "        #  TODO: TODO: Design your CNN forward method                 #\n",
        "        ######################################################################################\n",
        "        \n",
        "        features = self.pretrained(images) # [32, 2048, 7, 7]\n",
        "        features = features.view(features.shape[0] ,features.shape[1], -1) # [batch_size, num_feature_maps, img_size_1*img_size_2]\n",
        "        #print(features.shape) # [32, 2048, 49]\n",
        "        features = features.permute(0, 2, 1) # [32, 49, 2048]\n",
        "\n",
        "        ########################################################################\n",
        "        #             End of your code              #\n",
        "        ########################################################################\n",
        "\n",
        "        return features #(batch_size, num_pixels, encoder_dim) ,if pretrained = resnet50: (batch_size ,49, 2048)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd01de49",
      "metadata": {
        "id": "cd01de49"
      },
      "source": [
        "<a id='step4'></a>\n",
        "## Step 3(Your Turn):  Implement Attention Module(15 points)\n",
        "\n",
        "In this part, you should design Attention Module.\n",
        "\n",
        "In the code cell below, the output `alpha` should be a PyTorch tensor with size `[batch_size, num_pixels]`, and the output `attention_weights` should be a PyTorch tensor with size `[batch_size, encoder_dim]`.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00e63072",
      "metadata": {
        "id": "00e63072"
      },
      "outputs": [],
      "source": [
        "#Bahdanau Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.attention_dim = attention_dim\n",
        "\n",
        "        ######################################################################################\n",
        "        #  TODO: Design your Attention layers                       #\n",
        "        #  The layers should be some Linear layers                   #\n",
        "        ######################################################################################\n",
        "        \n",
        "        self.encoder_att = nn.Linear(encoder_dim, self.attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_att = nn.Linear(decoder_dim, self.attention_dim)  # linear layer to transform decoder's output\n",
        "        \n",
        "        \n",
        "        self.full_att = nn.Linear(attention_dim, 1) # linear layer to calculate values to be softmax-ed\n",
        "\n",
        "        ########################################################################\n",
        "        #             End of your code              #\n",
        "        ########################################################################\n",
        "        \n",
        "    def forward(self, features, hidden_state):\n",
        "      # features (batch_size, num_pixels, encoder_dim)\n",
        "      # hidden_state (batch_size, decoder_dim)\n",
        "        \n",
        "        ######################################################################################\n",
        "        #  TODO: Design the forward method of your Attention module         #\n",
        "        ######################################################################################\n",
        "\n",
        "        atten_1 = self.encoder_att(features) \n",
        "        #(batch_size, num_pixels, attention_dim)\n",
        "        atten_2 = self.decoder_att(hidden_state)\n",
        "        #(batch_size, attention_dim)\n",
        "        \n",
        "        combined_atten = self.full_att(F.relu(atten_1 + atten_2.unsqueeze(1))).squeeze(2)\n",
        "        #(batch_size, num_pixels, attemtion_dim) \n",
        "        \n",
        "\n",
        "        alpha = F.softmax(combined_atten, dim = 1)     \n",
        "        #(batch_size, num_pixels)\n",
        "        \n",
        "        attention_weights = (features * alpha.unsqueeze(2)).sum(dim=1) # context\n",
        "        #(batch_size, features_dim)\n",
        "\n",
        "        ########################################################################\n",
        "        #             End of your code              #\n",
        "        ########################################################################\n",
        "        \n",
        "        return alpha, attention_weights   # alpha:(batch_size, num_pixels)  attention_weights:#(batch_size, features_dim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nsiKwVeTYaj1",
      "metadata": {
        "id": "nsiKwVeTYaj1"
      },
      "source": [
        "<a id='step4'></a>\n",
        "## Step 4(Your Turn):  Implement the Attention RNN Decoder(10 points)\n",
        "\n",
        "In this part, you should design your Attention RNN Decoder.\n",
        "\n",
        "For Attention RNN Decoder, the iteration is performed manually in a for loop with a PyTorch `LSTMCell` instead of iterating automatically without a loop with a PyTorch `LSTM`. This is because we need to execute the Attention mechanism between each decode step. An `LSTMCell` is a single timestep operation, whereas an `LSTM` would iterate over multiple timesteps continously and provide all outputs at once.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf3f9b59",
      "metadata": {
        "id": "cf3f9b59"
      },
      "outputs": [],
      "source": [
        "#Attention Decoder\n",
        "class DecoderAttentionRNN(nn.Module):\n",
        "    def __init__(self,embed_size, vocab_size, attention_dim, encoder_dim, decoder_dim, drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        \n",
        "        self.vocab_size = vocab_size\n",
        "        self.attention_dim = attention_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        \n",
        "        ######################################################################################\n",
        "        #  TODO: Design your DecoderAttentionRNN layers                 #\n",
        "        #  The LSTM layer should be nn.LSTMCell instead of nn.LSTM         #\n",
        "        ######################################################################################\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)\n",
        "        \n",
        "        \n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim) \n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  \n",
        "        self.lstm_cell = nn.LSTMCell(embed_size + encoder_dim, decoder_dim, bias=True)\n",
        "        ########################################################################\n",
        "        #             End of your code              #\n",
        "        ########################################################################\n",
        "        \n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)\n",
        "        self.fcn = nn.Linear(decoder_dim, vocab_size)\n",
        "        \n",
        "        self.drop = nn.Dropout(drop_prob)\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        # encoder_out (batch_size, num_pixels, encoder_dim)\n",
        "\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        # mean_encoder_out (batch_size, encoder_dim)\n",
        "        \n",
        "        ############################################################################################\n",
        "        #  TODO: Use the init_h & init_c layers to get the hidden_state and the cell #\n",
        "        ############################################################################################\n",
        "\n",
        "        h = self.init_h(mean_encoder_out)\n",
        "        # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        # (batch_size, decoder_dim)\n",
        "\n",
        "        ########################################################################\n",
        "        #             End of your code              #\n",
        "        ########################################################################\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        \n",
        "        ######################################################################################\n",
        "        #  TODO: Design the forward method of your Attention RNNDecoder       #\n",
        "        #  You shoud first embed the captions                      #\n",
        "        #  Then draw the hidden_state and the cell from the image features    #\n",
        "        ######################################################################################\n",
        "        \n",
        "        #vectorize the caption\n",
        "        embeds = self.embedding(captions)\n",
        "        \n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(features) \n",
        "        # both are the size of (batch_size, decoder_dim) \n",
        "\n",
        "        ########################################################################\n",
        "        #             End of your code              #\n",
        "        ########################################################################\n",
        "        \n",
        "        #get the seq length to iterate\n",
        "        seq_length = len(captions[0])-1 #Exclude the last one\n",
        "        batch_size = captions.size(0)\n",
        "        num_features = features.size(1)\n",
        "        preds = torch.zeros(batch_size, seq_length, self.vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, seq_length, num_features).to(device)\n",
        "        \n",
        "        for s in range(seq_length):\n",
        "            \n",
        "            ######################################################################################\n",
        "            #  TODO: Get the attention output from the attention module         #\n",
        "            ######################################################################################\n",
        "            alpha, attention_weight = self.attention(features, h)\n",
        "            ########################################################################\n",
        "            #             End of your code              #\n",
        "            ########################################################################\n",
        "            \n",
        "            lstm_input = torch.cat((embeds[:, s], attention_weight), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "                    \n",
        "            output = self.fcn(self.drop(h))\n",
        "            \n",
        "            preds[:,s] = output\n",
        "            alphas[:,s] = alpha  \n",
        "        \n",
        "        \n",
        "        return preds, alphas\n",
        "\n",
        "\n",
        "    #Testing Phase\n",
        "    def generate_caption(self, features, max_len=20, vocab=None):\n",
        "        # Inference part\n",
        "        # Given the image features generate the captions\n",
        "        \n",
        "        batch_size = features.size(0)\n",
        "\n",
        "        ######################################################################################\n",
        "        #  TODO: Initialize the LSTM state with the same method in training phase            #\n",
        "        #  draw the hidden_state and the cell from the image features                        #\n",
        "        ######################################################################################\n",
        "        \n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(features)   \n",
        "        # both are the size of (batch_size, decoder_dim)\n",
        "\n",
        "        ########################################################################\n",
        "        #             End of your code              #\n",
        "        ########################################################################\n",
        "        \n",
        "        alphas = []\n",
        "        \n",
        "        #starting input\n",
        "        word = torch.tensor(vocab.stoi['<SOS>']).view(1,-1).to(device)\n",
        "        embeds = self.embedding(word)\n",
        "        \n",
        "        captions = []\n",
        "        \n",
        "        for i in range(max_len):\n",
        "\n",
        "            ######################################################################################\n",
        "            #  TODO: Get the attention output from the attention module         #\n",
        "            ######################################################################################\n",
        "            alpha, attention_weight = self.attention(features, h)\n",
        "            ########################################################################\n",
        "            #             End of your code              #\n",
        "            ######################################################################## \n",
        "            \n",
        "            #store the apla score\n",
        "            alphas.append(alpha.cpu().detach().numpy())\n",
        "            \n",
        "            lstm_input = torch.cat((embeds[:, 0], attention_weight), dim=1)\n",
        "            h, c = self.lstm_cell(lstm_input, (h, c))\n",
        "            output = self.fcn(self.drop(h))\n",
        "            output = output.view(batch_size, -1)      \n",
        "            \n",
        "            #select the word with most val\n",
        "            predicted_word_idx = output.argmax(dim=1)\n",
        "            \n",
        "            #save the generated word\n",
        "            captions.append(predicted_word_idx.item())\n",
        "            \n",
        "            #end if <EOS detected>\n",
        "            if vocab.itos[predicted_word_idx.item()] == \"<EOS>\":\n",
        "                break\n",
        "            \n",
        "            #send generated word as the next caption\n",
        "            embeds = self.embedding(predicted_word_idx.unsqueeze(0))\n",
        "        \n",
        "        #covert the vocab idx to words and return sentence\n",
        "        return [vocab.itos[idx] for idx in captions], alphas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88984b0",
      "metadata": {
        "id": "c88984b0"
      },
      "source": [
        "<a id='step4'></a>\n",
        "## Step 5: Combine the Encoder with Decoder\n",
        "\n",
        "In this part, you should combline the encoder and the decoder you designed above. To do that, you simply have to take the outputs of your encoder and the captions and send them into the decoder.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04544f5a",
      "metadata": {
        "id": "04544f5a"
      },
      "outputs": [],
      "source": [
        "class CNNtoAttentionRNN(nn.Module):\n",
        "    def __init__(self, embed_size, vocab_size, attention_dim, encoder_dim, decoder_dim, drop_prob=0.3):\n",
        "        super().__init__()\n",
        "        self.encoder = EncoderAttentionCNN()\n",
        "        self.decoder = DecoderAttentionRNN(\n",
        "            embed_size = embed_size,\n",
        "            vocab_size = len(dataset.vocab),\n",
        "            attention_dim = attention_dim,\n",
        "            encoder_dim = encoder_dim,\n",
        "            decoder_dim = decoder_dim\n",
        "        )\n",
        "        \n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2QkDsWHPKMo",
      "metadata": {
        "id": "b2QkDsWHPKMo"
      },
      "source": [
        "#Traning Process"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4114ba79",
      "metadata": {
        "id": "4114ba79"
      },
      "source": [
        "<a id='step5'></a>\n",
        "## Step 6: Training Settings\n",
        "\n",
        "You can modify some hyperparameters to improve the performance of you model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e1f277d",
      "metadata": {
        "id": "4e1f277d"
      },
      "outputs": [],
      "source": [
        "#Hyperparams\n",
        "embed_size=300\n",
        "vocab_size = len(dataset.vocab)\n",
        "attention_dim=256\n",
        "encoder_dim=2048\n",
        "decoder_dim=512\n",
        "learning_rate = 3e-4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4b6cfa",
      "metadata": {
        "id": "0c4b6cfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "2eb19ce44b8c4730a062faf811d83d25",
            "8426f355c3d44647877724eb6bfa5455",
            "58f56ea838424718b5ae46706fa4e3a6",
            "adcd0316d8eb46ea9fd21141c2124af7",
            "b99295dc2a0847a7861845a48573be3f",
            "3236b07daf9d47a185d1860680fc3f72",
            "ce5706767a714af0b1a7e58e5094131b",
            "9f140906cdac4a97ab8b7124f840a881",
            "7ee525106db3492fb2116796fcda41d8",
            "201c1854b3344be699a37ce9b519db59",
            "3899f4900cfb49dbafb97929b6bcbc7b"
          ]
        },
        "outputId": "17ef6997-e741-4f20-d76a-05a086c34a53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNeXt101_32X8D_Weights.IMAGENET1K_V1`. You can also use `weights=ResNeXt101_32X8D_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth\" to /root/.cache/torch/hub/checkpoints/resnext101_32x8d-8ba56ff5.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0.00/340M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2eb19ce44b8c4730a062faf811d83d25"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#init model\n",
        "model = CNNtoAttentionRNN(\n",
        "    embed_size=300,\n",
        "    vocab_size = len(dataset.vocab),\n",
        "    attention_dim=256,\n",
        "    encoder_dim=2048,\n",
        "    decoder_dim=512\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=dataset.vocab.stoi[\"<PAD>\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e28c91",
      "metadata": {
        "id": "48e28c91"
      },
      "source": [
        "<a id='step6'></a>\n",
        "## Step 7: Train Your Model\n",
        "\n",
        "Each epoch would probably take over an hour to train on Colab, so start early."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c98d58b",
      "metadata": {
        "id": "2c98d58b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d46a74fb-5f02-4e16-c0c0-953cfabb2fae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 19%|█▉        | 245/1265 [6:39:15<27:54:50, 98.52s/it]"
          ]
        }
      ],
      "source": [
        "num_epochs = 2\n",
        "#It takes about 3.75 hours to train 1 epoch on colab\n",
        "print_every = 100\n",
        "\n",
        "for epoch in range(1,num_epochs+1):   \n",
        "    \n",
        "    for idx, (image, captions) in tqdm(enumerate(train_loader), total=len(train_loader), leave=False):\n",
        "        image, captions = image.to(device), captions.to(device)\n",
        "\n",
        "        # Zero the gradients.\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        captions = captions.permute(1, 0)\n",
        "        \n",
        "        # Feed forward\n",
        "        outputs, attentions = model(image, captions)\n",
        "\n",
        "        # Calculate the batch loss.\n",
        "        targets = captions[:,1:]\n",
        "        loss = criterion(outputs.view(-1, vocab_size), targets.reshape(-1))\n",
        "        \n",
        "        # Backward pass.\n",
        "        loss.backward()\n",
        "\n",
        "        # Update the parameters in the optimizer.\n",
        "        optimizer.step()\n",
        "\n",
        "#         if (idx+1)%print_every == 0:\n",
        "#             print(\"Epoch: {} loss: {:.5f}\".format(epoch,loss.item()))\n",
        "            \n",
        "    stat_vals = 'Epochs [%d/%d] ------- Loss [%.4f] ' %( epoch, num_epochs, loss.item() )\n",
        "    print(stat_vals)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mzZZRyKKPQap",
      "metadata": {
        "id": "mzZZRyKKPQap"
      },
      "source": [
        "#Check The Results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bcc7f8b",
      "metadata": {
        "id": "6bcc7f8b"
      },
      "source": [
        "<a id='step7'></a>\n",
        "## Step 8: Simple Test\n",
        "\n",
        "This part is for the testing. After training, run the codes below to see some example captioning output. Note that since we only train for few epochs, the results might not be good. If you want to improve the result, you can try to train for more epachs, though it might takes a couple of hours."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "234d449c",
      "metadata": {
        "id": "234d449c"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "#generate caption\n",
        "def get_caps_from(features_tensors):\n",
        "    #generate the caption\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        features = model.encoder(features_tensors.to(device))\n",
        "        caps, alphas = model.decoder.generate_caption(features, vocab=dataset.vocab)\n",
        "        caption = ' '.join(caps)\n",
        "        show_image(features_tensors[0], title = caption)\n",
        "    \n",
        "    return caps, alphas\n",
        "\n",
        "#Show attention\n",
        "def plot_attention(img, result, attention_plot):\n",
        "    #untransform\n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224 \n",
        "    img[2] = img[2] * 0.225 \n",
        "    img[0] += 0.485 \n",
        "    img[1] += 0.456 \n",
        "    img[2] += 0.406\n",
        "    \n",
        "    img = img.numpy().transpose((1, 2, 0))\n",
        "    temp_image = img\n",
        "\n",
        "    fig = plt.figure(figsize=(15, 15))\n",
        "\n",
        "    len_result = len(result)\n",
        "    \n",
        "    for l in range(len_result):\n",
        "\n",
        "        temp_att = attention_plot[l].reshape(7,7)\n",
        "        \n",
        "        ax = fig.add_subplot(len_result//2, len_result//2, l+1)\n",
        "        ax.set_title(result[l])\n",
        "        img = ax.imshow(temp_image)\n",
        "        ax.imshow(temp_att, cmap='gray', alpha=0.8, extent=img.get_extent())      \n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "def show_image(img, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    \n",
        "    #unnormalize \n",
        "    img[0] = img[0] * 0.229\n",
        "    img[1] = img[1] * 0.224 \n",
        "    img[2] = img[2] * 0.225 \n",
        "    img[0] += 0.485 \n",
        "    img[1] += 0.456 \n",
        "    img[2] += 0.406\n",
        "    \n",
        "    img = img.numpy().transpose((1, 2, 0)) \n",
        "    \n",
        "    plt.imshow(img)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b20d019",
      "metadata": {
        "id": "8b20d019"
      },
      "outputs": [],
      "source": [
        "#show any 1\n",
        "dataiter = iter(train_loader)\n",
        "images, _ = next(dataiter)\n",
        "\n",
        "img = images[0].detach().clone()\n",
        "img1 = images[0].detach().clone()\n",
        "caps, alphas = get_caps_from(img.unsqueeze(0))\n",
        "\n",
        "plot_attention(img1, caps, alphas)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m0iOUPX0appM",
      "metadata": {
        "id": "m0iOUPX0appM"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2eb19ce44b8c4730a062faf811d83d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8426f355c3d44647877724eb6bfa5455",
              "IPY_MODEL_58f56ea838424718b5ae46706fa4e3a6",
              "IPY_MODEL_adcd0316d8eb46ea9fd21141c2124af7"
            ],
            "layout": "IPY_MODEL_b99295dc2a0847a7861845a48573be3f"
          }
        },
        "8426f355c3d44647877724eb6bfa5455": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3236b07daf9d47a185d1860680fc3f72",
            "placeholder": "​",
            "style": "IPY_MODEL_ce5706767a714af0b1a7e58e5094131b",
            "value": "100%"
          }
        },
        "58f56ea838424718b5ae46706fa4e3a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9f140906cdac4a97ab8b7124f840a881",
            "max": 356082095,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7ee525106db3492fb2116796fcda41d8",
            "value": 356082095
          }
        },
        "adcd0316d8eb46ea9fd21141c2124af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_201c1854b3344be699a37ce9b519db59",
            "placeholder": "​",
            "style": "IPY_MODEL_3899f4900cfb49dbafb97929b6bcbc7b",
            "value": " 340M/340M [00:02&lt;00:00, 162MB/s]"
          }
        },
        "b99295dc2a0847a7861845a48573be3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3236b07daf9d47a185d1860680fc3f72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce5706767a714af0b1a7e58e5094131b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f140906cdac4a97ab8b7124f840a881": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ee525106db3492fb2116796fcda41d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "201c1854b3344be699a37ce9b519db59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3899f4900cfb49dbafb97929b6bcbc7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}