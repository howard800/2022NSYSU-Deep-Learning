{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/howard800/2022NSYSU-Deep-Learning/blob/main/A5_sentiment_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6Z1Snuk7rIK"
      },
      "source": [
        "# MIS 583 Assignment 5: Text Sentiment Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSwr9MgZogRZ"
      },
      "source": [
        "Before we start, please put your name and SID in following format: <br>\n",
        ": LASTNAME Firstname, ?00000000   //   e.g.) 陳耀融, M094020099"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Your Answer:**   \n",
        "Hi I'm 王渙鈞, M114020052"
      ],
      "metadata": {
        "id": "6DzsjuDhlz_k"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5d-Zzebq7rIM"
      },
      "source": [
        "## Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kc9gd_Wk7rIN"
      },
      "source": [
        "**Sentiment Classification** is an automated process of identifying opinions in text and labeling them as positive, negative, or neutral, based on the emotions customers express within them.\n",
        "\n",
        "In this assignment, you need to train a recurrent neural network (RNN)\n",
        "or fine-tune a pre-trained language model (e.g., BERT) to predict the\n",
        "sentiment of given tweet."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giUId1Naqacs",
        "tags": []
      },
      "source": [
        "##  Versions of used packages\n",
        "\n",
        "We will check PyTorch version to make sure everything work properly.  \n",
        "We use `python==3.7.14`, `torch==1.12.1+cu113` and `torchvision==0.13.1+cu113`.  \n",
        "This is the default version in Google Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vuw-gNvjqcYe"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import torch\n",
        "import torchvision\n",
        "print('python', sys.version.split('\\n')[0])\n",
        "print('torch', torch.__version__)\n",
        "print('torchvision', torchvision.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a4s_a5D7rIR"
      },
      "source": [
        "## Loading Model and Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPUkTbnL7rIR"
      },
      "source": [
        "First, we are going to talk about the model.  \n",
        "HuggingFace team have created an amazing framework called \"transformers\" for NLP tasks.  \n",
        "It havs many State-of-the-art Machine Learning for Pytorch, TensorFlow, and JAX.  \n",
        "\n",
        "To start with this package, follow [this link to installation and basic tutorial](https://pytorch.org/hub/huggingface_pytorch-transformers/)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# you might need some additional installations there\n",
        "!echo happy installation\n",
        "!pip -V"
      ],
      "metadata": {
        "id": "rK0ouXa09pDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmGCAevi7rIS"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "#########################################################################\n",
        "#            Loading tokenizer and model from transformer               #\n",
        "#########################################################################\n",
        "# from transformers import xxx\n",
        "\n",
        "bert_type = 'xxx'\n",
        "\n",
        "\n",
        "# ---------- 1. load from torch.hub ----------\n",
        "tokenizer = torch.hub.load(xxx, 'tokennnnnizer', bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = torch.hub.load(xxx, 'modddddddddel', bert_type)\n",
        "\n",
        "# ---------- 2. load from installed huggingface ----------\n",
        "tokenizer = XXXTokenizer.from_pretrained(bert_type)\n",
        "\n",
        "# create a Bert-extended task (classification)\n",
        "model = XXXForSequenceClassification.from_pretrained(bert_type)\n",
        "\n",
        "\n",
        "\n",
        "# finetune from the output from bert to your task\n",
        "model.classifier = nn.Linear(768, 3, bias=True)\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiMThsYeDa2O"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "## How to Get Data\n",
        "\n",
        "Please open the file `twitter_sentiment.zip`, creat shortcut to your Google Drive.\n",
        "\n",
        "1. open [LINK of Google Drive](https://drive.google.com/file/d/14SdYQ6FSowQnWVCd4dP12jQMjX9-YJIh/view?usp=sharing)\n",
        "2. Click \"Add shortcut to Drive\" in the top-right corner.\n",
        "3. Select the location where you want to place the shortcut.\n",
        "4. Click Add shortcut.\n",
        "\n",
        "After above procedures, we have a shortcut of zip file of dataset.  \n",
        "We can access this in colab after granting the permission of Google Drive.\n",
        "\n",
        "---\n",
        "\n",
        "請先到共用雲端硬碟將檔案 `twitter_sentiment.zip`，建立捷徑到自己的雲端硬碟中。\n",
        "\n",
        "> 操作步驟\n",
        "1. 點開雲端[連結](https://drive.google.com/file/d/14SdYQ6FSowQnWVCd4dP12jQMjX9-YJIh/view?usp=sharing)\n",
        "2. 點選右上角「新增雲端硬碟捷徑」\n",
        "3. 點選「我的雲端硬碟」\n",
        "4. 點選「新增捷徑」\n",
        "\n",
        "完成以上流程會在你的雲端硬碟中建立一個檔案的捷徑，接著我們在colab中取得權限即可使用。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jBY0b6zxI0r9"
      },
      "source": [
        "1. Executing the below code which will provide you with an authentication link\n",
        "2. Open the link\n",
        "3. Choose the Google account whose Drive you want to mount\n",
        "4. Allow Google Drive Stream access to your Google Account\n",
        "5. Copy the code displayed, paste it in the text box as shown below, and press Enter\n",
        "![](https://i1.wp.com/neptune.ai/wp-content/uploads/colab-code-copy.png?resize=512%2C102&ssl=1)\n",
        "\n",
        "Might be some outdated. Don't need the code right now, I think.  \n",
        "\n",
        "Finish!\n",
        "\n",
        "---\n",
        "\n",
        "執行此段後點選出現的連結，允許授權後，複製授權碼，貼在空格中後按下ENTER，即完成與雲端硬碟連結。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "lZnFgi5i_2oA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqO8DiB6VRQZ"
      },
      "source": [
        "## Unzip Data\n",
        "\n",
        "Unzip `twitter_sentiment.zip`, there are 3 csv files.\n",
        "\n",
        "- `train.csv`, `test.csv` and `val.csv`\n",
        "\n",
        "There are **10248** datas in training set.  \n",
        "There are **1461** datas in validation set.  \n",
        "There are **2928** datas in testing set.  \n",
        "\n",
        "---\n",
        "\n",
        "解壓縮 `twitter_sentiment.zip` 後可以發現裡面有兩個資料夾和三個csv檔。\n",
        "\n",
        "- `train.csv`, `test.csv` and `val.csv`\n",
        "\n",
        "Training set 有 **10248** 筆資料.  \n",
        "Validation set 有 **1461** 筆資料.  \n",
        "Testing set 有 **2928** 筆資料.  \n",
        "\n",
        "注意: 若有另外設定存放在雲端硬碟中的路徑，請記得本處路徑也須做更動。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq ./drive/MyDrive/twitter_sentiment.zip"
      ],
      "metadata": {
        "id": "OSlTMdxf8Zd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wf5GXTme7rIT"
      },
      "outputs": [],
      "source": [
        "# Utility function to extract text and label from csv file\n",
        "def get_texts(f_name='./twitter_sentiment', mode='train'):\n",
        "    text_list = []\n",
        "    label_list = []\n",
        "\n",
        "    f_path = os.path.join(f_name, '{}.csv'.format(mode))\n",
        "    with open(f_path) as f:\n",
        "        reader = csv.DictReader(f)\n",
        "        for line in reader:\n",
        "            text_list.append(line['text'])\n",
        "            if mode != 'test':\n",
        "                label_list.append(int(line['sentiment_label']))\n",
        "\n",
        "    return text_list, label_list"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4PR-BI3gHsnq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fpY0ZrK7rIV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import csv\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, f_name='./twitter_sentiment', mode='train'):\n",
        "        self.mode = mode\n",
        "\n",
        "        text_list, label_list = get_texts(f_name, mode)\n",
        "        print('mode', mode, 'has', len(text_list), 'datas')\n",
        "        text_list = tokenizer(text_list,\n",
        "                             truncation=True, padding=True,\n",
        "                             return_tensors='pt')\n",
        "\n",
        "        self.text_list = text_list['input_ids']\n",
        "        self.mask_list = text_list['attention_mask']\n",
        "\n",
        "        self.label_list = label_list\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text_list[idx]\n",
        "        mask = self.mask_list[idx]\n",
        "        if self.mode == 'test':\n",
        "            return text, mask\n",
        "        label = torch.tensor(self.label_list[idx])\n",
        "        return text, mask, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCmM4FSw7rIW"
      },
      "outputs": [],
      "source": [
        "dataset_train = TwitterDataset(mode='train')\n",
        "dataset_val = TwitterDataset(mode='val')\n",
        "dataset_test = TwitterDataset(mode='test')\n",
        "\n",
        "batch_size = 64\n",
        "train_data = DataLoader(dataset_train, batch_size=batch_size,\n",
        "                       shuffle=True)\n",
        "val_data = DataLoader(dataset_val, batch_size=batch_size // 2,\n",
        "                       shuffle=False)\n",
        "test_data = DataLoader(dataset_test, batch_size=batch_size // 2,\n",
        "                       shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bqkvofHc7rIY"
      },
      "outputs": [],
      "source": [
        "t = tokenizer.convert_ids_to_tokens(dataset_train[0][0])\n",
        "print('token', t)\n",
        "print('token to s', tokenizer.convert_tokens_to_string(t))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DxZrfCqW7rIY"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda')\n",
        "\n",
        "from torch import nn\n",
        "from transformers import AdamW\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpwgE2Gd7rIZ"
      },
      "source": [
        "## Utility Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlaiAZAD7rIa"
      },
      "outputs": [],
      "source": [
        "def accuracy(raw_preds, y):\n",
        "    preds = raw_preds.argmax(dim=1)\n",
        "    acc = (preds == y).sum()\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmc_Gms97rIa"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_loss_list = []\n",
        "val_loss_list = []\n",
        "\n",
        "def train(model, data, optimizer, criterion):\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Testing process                              #\n",
        "        #########################################################################\n",
        "        # 1. clean the gradients of optimizer\n",
        "        # 2. put correct variables into model\n",
        "        # 3. get prediction\n",
        "        # 4. Evalutate by criterion and accuracy\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        train_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "def test(model, data, criterion, log_loss=False):\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    total = 0\n",
        "    for text, mask, label in tqdm(data, total=len(data)):\n",
        "        text = text.to(device)\n",
        "        mask = mask.to(device)\n",
        "        label = label.to(device)\n",
        "\n",
        "        #########################################################################\n",
        "        #                          Training process                             #\n",
        "        #########################################################################\n",
        "        # 1. put correct variables into model\n",
        "        # 2. get prediction\n",
        "        # 3. Evalutate by criterion and accuracy\n",
        "        #########################################################################\n",
        "        #                          End of your code                             #\n",
        "        #########################################################################\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        if log_loss:\n",
        "            val_loss_list.append(loss.item())\n",
        "        epoch_acc += acc.item()\n",
        "        total += len(text)\n",
        "    return epoch_loss / total, epoch_acc / total\n",
        "\n",
        "# class for monitoring train and test acc/loss\n",
        "class Meter:\n",
        "    def __init__(self):\n",
        "        self.train_loss_list = []\n",
        "        self.train_acc_list = []\n",
        "        self.val_loss_list = []\n",
        "        self.val_acc_list = []\n",
        "\n",
        "    def update(self, train_loss, train_acc, val_loss, val_acc):\n",
        "        self.train_loss_list.append(train_loss)\n",
        "        self.train_acc_list.append(train_acc)\n",
        "        self.val_loss_list.append(val_loss)\n",
        "        self.val_acc_list.append(val_acc)\n",
        "\n",
        "    def plot(self):\n",
        "        x = range(len(self.train_loss_list))\n",
        "        plt.plot(x, self.train_loss_list)\n",
        "        plt.plot(x, self.val_loss_list, color='r')\n",
        "        plt.legend(['train_loss', 'val_loss'])\n",
        "        plt.show()\n",
        "        plt.plot(x, self.train_acc_list)\n",
        "        plt.plot(x, self.val_acc_list, color='r')\n",
        "        plt.legend(['train_acc', 'val_acc'])\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExZyrKd57rIb"
      },
      "source": [
        "## Start Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVDe-fRe7rIc"
      },
      "outputs": [],
      "source": [
        "#########################################################################\n",
        "#                          Hyper-parameters                             #\n",
        "#########################################################################\n",
        "max_epoch = 3\n",
        "log_interval = 1\n",
        "best_acc = 0\n",
        "#########################################################################\n",
        "#                          End of your code                             #\n",
        "#########################################################################\n",
        "\n",
        "m = Meter()\n",
        "\n",
        "for epoch in range(1, max_epoch + 1):\n",
        "    train_loss, train_acc = train(model, train_data, optimizer, criterion)\n",
        "    val_loss, val_acc = test(model, val_data, criterion, log_loss=True)\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "        print('Epoch {} train_loss: {} train_acc: {}'.format(\n",
        "            epoch, train_loss, train_acc\n",
        "        ))\n",
        "        print('Epoch {} val_loss:  {} val_acc : {}'.format(\n",
        "            epoch, val_loss, val_acc\n",
        "        ))\n",
        "\n",
        "    m.update(train_loss, train_acc, val_loss, val_acc)\n",
        "\n",
        "    # model checkpoint\n",
        "    torch.save(model.state_dict(), 'ckpts/e{}.pt'.format(epoch))\n",
        "    if val_acc > best_acc:\n",
        "        best_model = model\n",
        "        best_acc = val_acc\n",
        "        print('-'*10, 'e', epoch, 'save best model', '-'*10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmtW58OR7rIc"
      },
      "outputs": [],
      "source": [
        "# plot them out\n",
        "m.plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcJcHf7n7rId"
      },
      "source": [
        "## Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf5UTlMZ7rId"
      },
      "outputs": [],
      "source": [
        "best_model.eval()\n",
        "\n",
        "total_out = []\n",
        "for text, mask in tqdm(test_data, total=len(test_data)):\n",
        "    text = text.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    output = best_model(text, mask)\n",
        "    pred = output.logits\n",
        "    pred = torch.argmax(pred, dim=1)\n",
        "    total_out.append(pred)\n",
        "\n",
        "total_out = torch.cat(total_out).cpu().numpy().tolist()\n",
        "\n",
        "with open('pred.csv', 'w') as f:\n",
        "    f.write('index,sentiment_label\\n')\n",
        "    for i, pred in enumerate(total_out):\n",
        "        f.write('{},{}\\n'.format(i, pred))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}